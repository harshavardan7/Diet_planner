{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b365cae-92e2-4fc6-aa98-32366db40288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5007 images belonging to 20 classes.\n",
      "Found 1262 images belonging to 20 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 1s/step - accuracy: 0.0801 - loss: 2.9335 - val_accuracy: 0.1458 - val_loss: 2.6800\n",
      "Epoch 2/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1875 - loss: 2.4906 - val_accuracy: 0.1429 - val_loss: 2.5447\n",
      "Epoch 3/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - accuracy: 0.1311 - loss: 2.7345 - val_accuracy: 0.1274 - val_loss: 2.7412\n",
      "Epoch 4/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1250 - loss: 2.8451 - val_accuracy: 0.1429 - val_loss: 2.9072\n",
      "Epoch 5/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - accuracy: 0.1305 - loss: 2.6840 - val_accuracy: 0.1579 - val_loss: 2.7006\n",
      "Epoch 6/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0625 - loss: 2.4998 - val_accuracy: 0.0714 - val_loss: 2.9059\n",
      "Epoch 7/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 1s/step - accuracy: 0.1553 - loss: 2.6384 - val_accuracy: 0.2404 - val_loss: 2.3767\n",
      "Epoch 8/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2188 - loss: 2.3609 - val_accuracy: 0.2143 - val_loss: 2.3234\n",
      "Epoch 9/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - accuracy: 0.2282 - loss: 2.4712 - val_accuracy: 0.2596 - val_loss: 2.3049\n",
      "Epoch 10/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3750 - loss: 2.3536 - val_accuracy: 0.1429 - val_loss: 2.6751\n",
      "Epoch 11/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - accuracy: 0.3000 - loss: 2.2795 - val_accuracy: 0.2804 - val_loss: 2.3530\n",
      "Epoch 12/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1875 - loss: 2.5057 - val_accuracy: 0.4286 - val_loss: 2.0577\n",
      "Epoch 13/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 1s/step - accuracy: 0.3256 - loss: 2.2055 - val_accuracy: 0.3029 - val_loss: 2.3488\n",
      "Epoch 14/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2812 - loss: 2.4454 - val_accuracy: 0.4286 - val_loss: 2.0337\n",
      "Epoch 15/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 1s/step - accuracy: 0.3433 - loss: 2.1349 - val_accuracy: 0.3870 - val_loss: 1.9996\n",
      "Epoch 16/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3750 - loss: 1.8214 - val_accuracy: 0.0714 - val_loss: 2.6380\n",
      "Epoch 17/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 2s/step - accuracy: 0.3862 - loss: 2.0048 - val_accuracy: 0.4399 - val_loss: 1.8278\n",
      "Epoch 18/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3125 - loss: 1.8731 - val_accuracy: 0.5714 - val_loss: 1.5808\n",
      "Epoch 19/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 2s/step - accuracy: 0.4138 - loss: 1.9195 - val_accuracy: 0.4391 - val_loss: 1.7985\n",
      "Epoch 20/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 1.6100 - val_accuracy: 0.4286 - val_loss: 2.2209\n",
      "Epoch 21/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 2s/step - accuracy: 0.4247 - loss: 1.8429 - val_accuracy: 0.4495 - val_loss: 1.7481\n",
      "Epoch 22/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3438 - loss: 1.9439 - val_accuracy: 0.5000 - val_loss: 1.4768\n",
      "Epoch 23/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.4523 - loss: 1.7765 - val_accuracy: 0.4535 - val_loss: 1.7363\n",
      "Epoch 24/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4062 - loss: 2.2023 - val_accuracy: 0.5000 - val_loss: 1.7293\n",
      "Epoch 25/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 2s/step - accuracy: 0.4728 - loss: 1.6791 - val_accuracy: 0.4840 - val_loss: 1.6532\n",
      "Epoch 26/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4688 - loss: 1.8444 - val_accuracy: 0.2143 - val_loss: 2.1855\n",
      "Epoch 27/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 2s/step - accuracy: 0.4834 - loss: 1.6524 - val_accuracy: 0.5144 - val_loss: 1.5482\n",
      "Epoch 28/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5625 - loss: 1.5947 - val_accuracy: 0.5714 - val_loss: 1.4747\n",
      "Epoch 29/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 2s/step - accuracy: 0.5130 - loss: 1.5997 - val_accuracy: 0.4848 - val_loss: 1.7100\n",
      "Epoch 30/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4688 - loss: 1.7915 - val_accuracy: 0.7857 - val_loss: 0.7030\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.4569 - loss: 1.7760\n",
      "Validation Accuracy: 0.4976228177547455\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Original data path\n",
    "data_path = r'C:\\Users\\17har\\Downloads\\archive (8)\\Food Classification'\n",
    "\n",
    "# Define paths for training and validation sets\n",
    "base_dir = 'food_data_split'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Create directories for train and validation\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "for class_name in os.listdir(data_path):\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "        train_images, validation_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Create class directories in train and validation directories\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)\n",
    "        \n",
    "        # Move training images\n",
    "        for image in train_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(train_dir, class_name, image)\n",
    "            shutil.copyfile(src, dst)\n",
    "        \n",
    "        # Move validation images\n",
    "        for image in validation_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(validation_dir, class_name, image)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "# Data generators for loading and augmenting data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35a0c2a-228d-45ef-818f-92fd32f266cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('food_classification_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56b5d247-4c35-4668-ab3e-bcd7ca68ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m17har\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m240.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m img_array \u001b[38;5;241m=\u001b[39m load_and_preprocess_image(img_path)\n\u001b[1;32m---> 35\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m predict_image_class(model, img_array, class_indices)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe image is classified as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 29\u001b[0m, in \u001b[0;36mpredict_image_class\u001b[1;34m(model, img_array, class_indices)\u001b[0m\n\u001b[0;32m     27\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img_array)\n\u001b[0;32m     28\u001b[0m predicted_class_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions)\n\u001b[1;32m---> 29\u001b[0m predicted_class_label \u001b[38;5;241m=\u001b[39m class_indices[predicted_class_index]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_class_label\n",
      "\u001b[1;31mKeyError\u001b[0m: 19"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "import numpy as np\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('food_classification_model.keras')\n",
    "\n",
    "# Define your class indices\n",
    "class_indices = {0: 'Class_0', 1: 'Class_1', 2: 'Class_2'}  # Replace with your actual class indices\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    try:\n",
    "        img = keras_image.load_img(img_path, target_size=(150, 150))\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array /= 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image_class(model, img_array, class_indices):\n",
    "    if img_array is None:\n",
    "        return \"Unknown\"\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class_label = class_indices[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Example usage\n",
    "img_path = r'C:\\Users\\17har\\OneDrive\\Desktop\\240.jpg'\n",
    "img_array = load_and_preprocess_image(img_path)\n",
    "predicted_class = predict_image_class(model, img_array, class_indices)\n",
    "print(f'The image is classified as: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dca25fa-bf77-4e1c-88d1-1acc091e3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"food_classification_model.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10bfb7b-2d61-46c8-8735-e46966d14cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\17har\\Downloads\\archive (8)\\Food Classification\\food_classification_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4a9b43d-7fd0-4444-aadf-c7886b7c9b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is classified as: Unknown with confidence 0.00\n",
      "Calories in the dish: 0\n",
      "Recommended daily calorie intake: 2555.5625\n",
      "Calorie difference: 2555.5625\n",
      "Dietary recommendation: You have remaining calories for the day. Consider consuming a balanced meal with proteins, carbohydrates, and fats.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image as keras_image  # Rename the module to avoid conflict\n",
    "import numpy as np\n",
    "\n",
    "# Load the model (replace 'path_to_your_model' with the actual path)\n",
    "model = tf.keras.models.load_model('food_classification_model.keras')\n",
    "\n",
    "# Dummy class_indices for demonstration purposes\n",
    "class_indices = {0: 'Pasta', 1: 'Salad', 2: 'Burger', 3: 'samosa'}  # Replace with your actual class indices\n",
    "\n",
    "# Calorie mapping for each class\n",
    "calorie_map = {\n",
    "    'Pasta': 300,\n",
    "    'Salad': 150,\n",
    "    'Burger': 500,\n",
    "    'samosa': 100\n",
    "}\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    try:\n",
    "        img = keras_image.load_img(img_path, target_size=(150, 150))  # Use renamed module\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        img_array /= 255.0  # Normalize to [0,1]\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def predict_image_class(model, img_array):\n",
    "    predictions = model(img_array)\n",
    "    return predictions\n",
    "\n",
    "def get_class_label_and_confidence(predictions, class_indices):\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class_label = class_indices.get(predicted_class_index, \"Unknown\")\n",
    "    prediction_confidence = predictions[0][predicted_class_index] if predicted_class_label != \"Unknown\" else 0\n",
    "    return predicted_class_label, prediction_confidence\n",
    "\n",
    "def calculate_calorie_difference(recommended_calories, consumed_calories):\n",
    "    return recommended_calories - consumed_calories\n",
    "\n",
    "def provide_dietary_recommendation(calorie_difference):\n",
    "    if calorie_difference > 0:\n",
    "        return \"You have remaining calories for the day. Consider consuming a balanced meal with proteins, carbohydrates, and fats.\"\n",
    "    elif calorie_difference < 0:\n",
    "        return \"You have exceeded your calorie intake for the day. Consider light snacks or low-calorie foods for the rest of the day.\"\n",
    "    else:\n",
    "        return \"You have met your calorie intake for the day. Maintain a balanced diet.\"\n",
    "\n",
    "# Example user info and model loading\n",
    "name = \"John Doe\"\n",
    "age = 30\n",
    "gender = \"male\"\n",
    "height = 175  # in cm\n",
    "weight = 70   # in kg\n",
    "activity_level = \"moderately active\"\n",
    "goal = \"maintain weight\"\n",
    "\n",
    "# Placeholder function to calculate recommended calories\n",
    "def get_recommended_calories(age, gender, height, weight, activity_level, goal):\n",
    "    # Calculate BMR\n",
    "    if gender.lower() == 'male':\n",
    "        bmr = 10 * weight + 6.25 * height - 5 * age + 5\n",
    "    else:\n",
    "        bmr = 10 * weight + 6.25 * height - 5 * age - 161\n",
    "    \n",
    "    # Calculate TDEE based on activity level\n",
    "    activity_factors = {\n",
    "        'sedentary': 1.2,\n",
    "        'lightly active': 1.375,\n",
    "        'moderately active': 1.55,\n",
    "        'very active': 1.725,\n",
    "        'super active': 1.9\n",
    "    }\n",
    "    tdee = bmr * activity_factors[activity_level.lower()]\n",
    "    \n",
    "    # Adjust TDEE based on goal\n",
    "    if goal.lower() == 'lose weight':\n",
    "        recommended_calories = tdee - 500\n",
    "    elif goal.lower() == 'gain weight':\n",
    "        recommended_calories = tdee + 500\n",
    "    else:\n",
    "        recommended_calories = tdee\n",
    "    \n",
    "    return recommended_calories\n",
    "\n",
    "# Get the user's recommended calories\n",
    "recommended_calories = get_recommended_calories(age, gender, height, weight, activity_level, goal)\n",
    "\n",
    "# Path to the image of the dish\n",
    "img_path = r'\"C:\\Users\\17har\\Downloads\\archive (8)\\Food Classification\\pizza\\252.jpg\"'\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_array = load_and_preprocess_image(img_path)\n",
    "\n",
    "# Predict the class of the dish\n",
    "predictions = predict_image_class(model, img_array)\n",
    "predicted_class, prediction_confidence = get_class_label_and_confidence(predictions, class_indices)\n",
    "\n",
    "# Get the calories for the predicted class\n",
    "consumed_calories = calorie_map.get(predicted_class, 0)\n",
    "\n",
    "# Calculate the calorie difference\n",
    "calorie_difference = calculate_calorie_difference(recommended_calories, consumed_calories)\n",
    "\n",
    "# Provide dietary recommendation\n",
    "dietary_recommendation = provide_dietary_recommendation(calorie_difference)\n",
    "\n",
    "print(f'The image is classified as: {predicted_class} with confidence {prediction_confidence:.2f}')\n",
    "print(f'Calories in the dish: {consumed_calories}')\n",
    "print(f'Recommended daily calorie intake: {recommended_calories}')\n",
    "print(f'Calorie difference: {calorie_difference}')\n",
    "print(f'Dietary recommendation: {dietary_recommendation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a82e3-8ba9-4ff7-91fc-7bc33ce0d4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
