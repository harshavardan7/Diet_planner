{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c2580d-ff14-4d88-abfa-625dc9dba4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5007 images belonging to 20 classes.\n",
      "Found 1262 images belonging to 20 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17har\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.0673 - loss: 2.9681 - val_accuracy: 0.1250 - val_loss: 2.7188\n",
      "Epoch 2/30\n",
      "\u001b[1m  1/156\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 287ms/step - accuracy: 0.0625 - loss: 2.9259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17har\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0625 - loss: 2.9259 - val_accuracy: 0.2143 - val_loss: 2.6342\n",
      "Epoch 3/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.1267 - loss: 2.7337 - val_accuracy: 0.1995 - val_loss: 2.5768\n",
      "Epoch 4/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1875 - loss: 2.6140 - val_accuracy: 0.1429 - val_loss: 2.3239\n",
      "Epoch 5/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 2s/step - accuracy: 0.1882 - loss: 2.5895 - val_accuracy: 0.2179 - val_loss: 2.5137\n",
      "Epoch 6/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3438 - loss: 2.2600 - val_accuracy: 0.0714 - val_loss: 2.4125\n",
      "Epoch 7/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.2204 - loss: 2.5143 - val_accuracy: 0.2163 - val_loss: 2.5240\n",
      "Epoch 8/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2812 - loss: 2.3413 - val_accuracy: 0.1429 - val_loss: 2.9208\n",
      "Epoch 9/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 2s/step - accuracy: 0.2608 - loss: 2.3792 - val_accuracy: 0.2949 - val_loss: 2.4082\n",
      "Epoch 10/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4062 - loss: 1.7637 - val_accuracy: 0.2857 - val_loss: 2.2657\n",
      "Epoch 11/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 2s/step - accuracy: 0.2931 - loss: 2.2614 - val_accuracy: 0.2468 - val_loss: 2.5013\n",
      "Epoch 12/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2500 - loss: 2.3831 - val_accuracy: 0.3571 - val_loss: 2.0374\n",
      "Epoch 13/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.3312 - loss: 2.1572 - val_accuracy: 0.3654 - val_loss: 2.0884\n",
      "Epoch 14/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.2188 - loss: 2.4600 - val_accuracy: 0.1429 - val_loss: 2.3333\n",
      "Epoch 15/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.3426 - loss: 2.1156 - val_accuracy: 0.3574 - val_loss: 2.0460\n",
      "Epoch 16/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2500 - loss: 2.2529 - val_accuracy: 0.2143 - val_loss: 2.5361\n",
      "Epoch 17/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.3829 - loss: 1.9868 - val_accuracy: 0.3822 - val_loss: 1.9259\n",
      "Epoch 18/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3750 - loss: 1.7881 - val_accuracy: 0.5714 - val_loss: 1.2757\n",
      "Epoch 19/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 1s/step - accuracy: 0.3982 - loss: 1.9046 - val_accuracy: 0.4175 - val_loss: 1.8432\n",
      "Epoch 20/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426us/step - accuracy: 0.4375 - loss: 1.7195 - val_accuracy: 0.4286 - val_loss: 2.1191\n",
      "Epoch 21/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1s/step - accuracy: 0.4382 - loss: 1.8355 - val_accuracy: 0.4575 - val_loss: 1.7666\n",
      "Epoch 22/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - accuracy: 0.3438 - loss: 1.9050 - val_accuracy: 0.6429 - val_loss: 1.6744\n",
      "Epoch 23/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 3s/step - accuracy: 0.4474 - loss: 1.7934 - val_accuracy: 0.4920 - val_loss: 1.6857\n",
      "Epoch 24/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.3750 - loss: 1.9892 - val_accuracy: 0.3571 - val_loss: 2.1949\n",
      "Epoch 25/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 3s/step - accuracy: 0.4606 - loss: 1.7299 - val_accuracy: 0.4728 - val_loss: 1.6883\n",
      "Epoch 26/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6250 - loss: 1.4247 - val_accuracy: 0.5714 - val_loss: 1.1695\n",
      "Epoch 27/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 3s/step - accuracy: 0.4744 - loss: 1.6907 - val_accuracy: 0.4920 - val_loss: 1.6323\n",
      "Epoch 28/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5000 - loss: 1.7094 - val_accuracy: 0.4286 - val_loss: 1.6412\n",
      "Epoch 29/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 3s/step - accuracy: 0.4911 - loss: 1.6063 - val_accuracy: 0.5096 - val_loss: 1.6546\n",
      "Epoch 30/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3750 - loss: 1.8211 - val_accuracy: 0.6429 - val_loss: 1.3305\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.5247 - loss: 1.6199\n",
      "Validation Accuracy: 0.503169596195221\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Original data path\n",
    "data_path = r'C:\\Users\\17har\\Downloads\\archive (8)\\Food Classification'\n",
    "\n",
    "# Define paths for training and validation sets\n",
    "base_dir = 'food_data_split'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Create directories for train and validation\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "for class_name in os.listdir(data_path):\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "        train_images, validation_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Create class directories in train and validation directories\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)\n",
    "        \n",
    "        # Move training images\n",
    "        for image in train_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(train_dir, class_name, image)\n",
    "            shutil.copyfile(src, dst)\n",
    "        \n",
    "        # Move validation images\n",
    "        for image in validation_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(validation_dir, class_name, image)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "# Data generators for loading and augmenting data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17434b2d-70b1-419a-b7a1-19dd5a107398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5007 images belonging to 20 classes.\n",
      "Found 1262 images belonging to 20 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17har\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.0673 - loss: 2.9681 - val_accuracy: 0.1250 - val_loss: 2.7188\n",
      "Epoch 2/30\n",
      "\u001b[1m  1/156\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 287ms/step - accuracy: 0.0625 - loss: 2.9259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17har\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0625 - loss: 2.9259 - val_accuracy: 0.2143 - val_loss: 2.6342\n",
      "Epoch 3/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.1267 - loss: 2.7337 - val_accuracy: 0.1995 - val_loss: 2.5768\n",
      "Epoch 4/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1875 - loss: 2.6140 - val_accuracy: 0.1429 - val_loss: 2.3239\n",
      "Epoch 5/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 2s/step - accuracy: 0.1882 - loss: 2.5895 - val_accuracy: 0.2179 - val_loss: 2.5137\n",
      "Epoch 6/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3438 - loss: 2.2600 - val_accuracy: 0.0714 - val_loss: 2.4125\n",
      "Epoch 7/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.2204 - loss: 2.5143 - val_accuracy: 0.2163 - val_loss: 2.5240\n",
      "Epoch 8/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2812 - loss: 2.3413 - val_accuracy: 0.1429 - val_loss: 2.9208\n",
      "Epoch 9/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 2s/step - accuracy: 0.2608 - loss: 2.3792 - val_accuracy: 0.2949 - val_loss: 2.4082\n",
      "Epoch 10/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4062 - loss: 1.7637 - val_accuracy: 0.2857 - val_loss: 2.2657\n",
      "Epoch 11/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 2s/step - accuracy: 0.2931 - loss: 2.2614 - val_accuracy: 0.2468 - val_loss: 2.5013\n",
      "Epoch 12/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2500 - loss: 2.3831 - val_accuracy: 0.3571 - val_loss: 2.0374\n",
      "Epoch 13/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.3312 - loss: 2.1572 - val_accuracy: 0.3654 - val_loss: 2.0884\n",
      "Epoch 14/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.2188 - loss: 2.4600 - val_accuracy: 0.1429 - val_loss: 2.3333\n",
      "Epoch 15/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.3426 - loss: 2.1156 - val_accuracy: 0.3574 - val_loss: 2.0460\n",
      "Epoch 16/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2500 - loss: 2.2529 - val_accuracy: 0.2143 - val_loss: 2.5361\n",
      "Epoch 17/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.3829 - loss: 1.9868 - val_accuracy: 0.3822 - val_loss: 1.9259\n",
      "Epoch 18/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3750 - loss: 1.7881 - val_accuracy: 0.5714 - val_loss: 1.2757\n",
      "Epoch 19/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 1s/step - accuracy: 0.3982 - loss: 1.9046 - val_accuracy: 0.4175 - val_loss: 1.8432\n",
      "Epoch 20/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426us/step - accuracy: 0.4375 - loss: 1.7195 - val_accuracy: 0.4286 - val_loss: 2.1191\n",
      "Epoch 21/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1s/step - accuracy: 0.4382 - loss: 1.8355 - val_accuracy: 0.4575 - val_loss: 1.7666\n",
      "Epoch 22/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - accuracy: 0.3438 - loss: 1.9050 - val_accuracy: 0.6429 - val_loss: 1.6744\n",
      "Epoch 23/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 3s/step - accuracy: 0.4474 - loss: 1.7934 - val_accuracy: 0.4920 - val_loss: 1.6857\n",
      "Epoch 24/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.3750 - loss: 1.9892 - val_accuracy: 0.3571 - val_loss: 2.1949\n",
      "Epoch 25/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 3s/step - accuracy: 0.4606 - loss: 1.7299 - val_accuracy: 0.4728 - val_loss: 1.6883\n",
      "Epoch 26/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6250 - loss: 1.4247 - val_accuracy: 0.5714 - val_loss: 1.1695\n",
      "Epoch 27/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 3s/step - accuracy: 0.4744 - loss: 1.6907 - val_accuracy: 0.4920 - val_loss: 1.6323\n",
      "Epoch 28/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5000 - loss: 1.7094 - val_accuracy: 0.4286 - val_loss: 1.6412\n",
      "Epoch 29/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 3s/step - accuracy: 0.4911 - loss: 1.6063 - val_accuracy: 0.5096 - val_loss: 1.6546\n",
      "Epoch 30/30\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3750 - loss: 1.8211 - val_accuracy: 0.6429 - val_loss: 1.3305\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.5247 - loss: 1.6199\n",
      "Validation Accuracy: 0.503169596195221\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Original data path\n",
    "data_path = r'C:\\Users\\17har\\Downloads\\archive (8)\\Food Classification'\n",
    "\n",
    "# Define paths for training and validation sets\n",
    "base_dir = 'food_data_split'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Create directories for train and validation\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "for class_name in os.listdir(data_path):\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "        train_images, validation_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Create class directories in train and validation directories\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)\n",
    "        \n",
    "        # Move training images\n",
    "        for image in train_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(train_dir, class_name, image)\n",
    "            shutil.copyfile(src, dst)\n",
    "        \n",
    "        # Move validation images\n",
    "        for image in validation_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(validation_dir, class_name, image)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "# Data generators for loading and augmenting data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c92cd3c-3cdf-4f16-b0b2-53738b3da9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('food_classification_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f5390a1-32b6-4fe7-a4b9-e03b6adcb22d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'class_indices.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load the class indices\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_indices.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m     class_indices \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Reverse the class indices dictionary to map indices to class labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'class_indices.json'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('food_classification_model.keras', compile=False)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load the class indices\n",
    "with open('class_indices.json', 'r') as f:\n",
    "    class_indices = json.load(f)\n",
    "\n",
    "# Reverse the class indices dictionary to map indices to class labels\n",
    "class_indices = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    try:\n",
    "        img = keras_image.load_img(img_path, target_size=(150, 150))\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array /= 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image_class(model, img_array, class_indices):\n",
    "    if img_array is None:\n",
    "        return \"Unknown\"\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class_label = class_indices.get(predicted_class_index, \"Unknown\")\n",
    "    return predicted_class_label\n",
    "\n",
    "# Calorie map for each class\n",
    "calorie_map = {\n",
    "    'samosa': 250,\n",
    "    'Pizza': 300,\n",
    "    'Burger': 500\n",
    "}\n",
    "\n",
    "# Function to calculate recommended calories\n",
    "def get_recommended_calories(age, gender, height, weight, activity_level, goal):\n",
    "    if gender.lower() == 'male':\n",
    "        bmr = 10 * weight + 6.25 * height - 5 * age + 5\n",
    "    else:\n",
    "        bmr = 10 * weight + 6.25 * height - 5 * age - 161\n",
    "\n",
    "    activity_factors = {\n",
    "        'sedentary': 1.2,\n",
    "        'lightly active': 1.375,\n",
    "        'moderately active': 1.55,\n",
    "        'very active': 1.725,\n",
    "        'super active': 1.9\n",
    "    }\n",
    "\n",
    "    tdee = bmr * activity_factors[activity_level.lower()]\n",
    "\n",
    "    if goal.lower() == 'lose weight':\n",
    "        recommended_calories = tdee - 500\n",
    "    elif goal.lower() == 'gain weight':\n",
    "        recommended_calories = tdee + 500\n",
    "    else:\n",
    "        recommended_calories = tdee\n",
    "\n",
    "    return recommended_calories\n",
    "\n",
    "# Function to calculate calorie difference\n",
    "def calculate_calorie_difference(recommended_calories, consumed_calories):\n",
    "    return recommended_calories - consumed_calories\n",
    "\n",
    "# Function to provide dietary recommendation\n",
    "def provide_dietary_recommendation(calorie_difference):\n",
    "    if calorie_difference > 0:\n",
    "        return \"You have remaining calories for the day. Consider consuming a balanced meal with proteins, carbohydrates, and fats.\"\n",
    "    elif calorie_difference < 0:\n",
    "        return \"You have exceeded your calorie intake for the day. Consider light snacks or low-calorie foods for the rest of the day.\"\n",
    "    else:\n",
    "        return \"You have met your calorie intake for the day. Maintain a balanced diet.\"\n",
    "\n",
    "# Example user info\n",
    "name = \"John Doe\"\n",
    "age = 30\n",
    "gender = \"male\"\n",
    "height = 175  # in cm\n",
    "weight = 70   # in kg\n",
    "activity_level = \"moderately active\"\n",
    "goal = \"maintain weight\"\n",
    "\n",
    "# Path to the image of the dish\n",
    "img_path = r'C:\\Users\\17har\\OneDrive\\Desktop\\240.jpg'\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_array = load_and_preprocess_image(img_path)\n",
    "\n",
    "# Predict the class of the dish\n",
    "predicted_class = predict_image_class(model, img_array, class_indices)\n",
    "\n",
    "# Get the calories for the predicted class\n",
    "consumed_calories = calorie_map.get(predicted_class, 0)\n",
    "\n",
    "# Get the user's recommended calories\n",
    "recommended_calories = get_recommended_calories(age, gender, height, weight, activity_level, goal)\n",
    "\n",
    "# Calculate the calorie difference\n",
    "calorie_difference = calculate_calorie_difference(recommended_calories, consumed_calories)\n",
    "\n",
    "# Provide dietary recommendation\n",
    "dietary_recommendation = provide_dietary_recommendation(calorie_difference)\n",
    "\n",
    "print(f'The image is classified as: {predicted_class}')\n",
    "print(f'Calories in the dish: {consumed_calories}')\n",
    "print(f'Recommended daily calorie intake: {recommended_calories}')\n",
    "print(f'Calorie difference: {calorie_difference}')\n",
    "print(f'Dietary recommendation: {dietary_recommendation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c29d7a-0e7f-482a-90d0-750d0792734e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
